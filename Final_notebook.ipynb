{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD UNIPROT DATA AND TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import urllib.request as request\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from utils import *\n",
    "from contextlib import closing\n",
    "from pathlib import Path\n",
    "import numpy as np \n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "from torch.optim import lr_scheduler\n",
    "import copy\n",
    "import time\n",
    "from torch.nn import Linear, GRU, Conv2d, Dropout, MaxPool2d, BatchNorm1d\n",
    "from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import re\n",
    "import tape\n",
    "from tape import ProteinBertModel\n",
    "from tape import ProteinConfig\n",
    "import regex as re\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "#Seed\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def write_test_set(dataset, data_path, glyc_type):\n",
    "    \"\"\"writes the test parition data to fasta and txt file to enable use in netO and netN models\"\"\"\n",
    "    inp_tokenizer = TAPETokenizer(vocab='iupac')\n",
    "    tar_tokenizer = TAPETokenizer(vocab='glycolysation')\n",
    "    seqs = list()\n",
    "    tars = list()\n",
    "    special_tokens = (\"<pad>\", \"<mask>\", \"<cls>\", \"<sep>\", \"<unk>\")\n",
    "    for i in range(1000):\n",
    "        as_protein = inp_tokenizer.convert_ids_to_tokens(dataset.inputs[i].data.numpy())\n",
    "        as_protein = \"\".join([x for x in as_protein if x not in special_tokens])\n",
    "        header = \">test_seq_{}\\n\".format(i + 1)\n",
    "        seq = header + as_protein\n",
    "        seqs.append(seq)\n",
    "\n",
    "        as_one_hot = tar_tokenizer.convert_ids_to_tokens(dataset.labels[i].data.numpy())\n",
    "        as_one_hot = np.array([x for x in as_one_hot if x not in special_tokens], dtype=int)\n",
    "        tars.append(as_one_hot)\n",
    "\n",
    "    with open(data_path / \"test_seqs_{}.fsa\".format(glyc_type), \"w\") as seq_file:\n",
    "        for seq, tar in zip(seqs, tars):\n",
    "            print(seq, end=\"\\n\", file=seq_file)\n",
    "    np.savez_compressed(data_path / \"test_tars_{}.npz\".format(glyc_type), tars)\n",
    "\n",
    "\n",
    "def write_token_dataset_to_original(dataset):\n",
    "    \"\"\"writes the tokenized parition data back to original dataset format\"\"\"\n",
    "    \n",
    "    inp_tokenizer = TAPETokenizer(vocab='iupac')\n",
    "    tar_tokenizer = TAPETokenizer(vocab='glycolysation')\n",
    "    seqs = list()\n",
    "    tars = list()\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        as_protein = inp_tokenizer.convert_ids_to_tokens(dataset.inputs[i].data.numpy())\n",
    "        seq = \"\".join([x for x in as_protein if x not in (\"<pad>\", \"<mask>\", \"<cls>\", \"<sep>\", \"<unk>\")])\n",
    "        seqs.append(seq)\n",
    "\n",
    "        as_one_hot = tar_tokenizer.convert_ids_to_tokens(dataset.labels[i].data.numpy())\n",
    "        special_tokens = (\"<pad>\", \"<mask>\", \"<cls>\", \"<sep>\", \"<unk>\")\n",
    "        as_one_hot = np.array([x for x in as_one_hot if x not in special_tokens], dtype=int)\n",
    "        tars.append(as_one_hot)\n",
    "        \n",
    "    return seqs, tars  \n",
    "\n",
    "\n",
    "def concat_data(inp, tar, inp_non, tar_non):\n",
    "    \"\"\"\n",
    "    concatenates glyco and non glyco protein data\n",
    "    \"\"\"\n",
    "    rand_idx = np.random.permutation(len(inp_non))[0:len(inp)]\n",
    "    inp_non = inp_non[rand_idx, :]\n",
    "    tar_non = tar_non[rand_idx, :]\n",
    "    inp = np.concatenate([inp, inp_non], axis=0)\n",
    "    tar = np.concatenate([tar, tar_non], axis=0)\n",
    "    return inp, tar\n",
    "\n",
    "\n",
    "def save_tokenized_data(inp, tar, save_path):\n",
    "    print(\"Saving tokenized dataset in train, val and prediction partitions, to {}\".format(save_path))\n",
    "    inp = torch.from_numpy(inp)\n",
    "    tar = torch.from_numpy(tar)\n",
    "    train, validation, prediction = construct_datasets(\n",
    "        inp, \n",
    "        tar, \n",
    "        Dataset, \n",
    "        p_train=0.8, \n",
    "        p_val=0.1, \n",
    "        p_test=0.1\n",
    "    )\n",
    "\n",
    "    save_train_inp = np.vstack([t[0].data.numpy() for t in train])\n",
    "    save_train_tar = np.vstack([t[1].data.numpy() for t in train])\n",
    "    save_val_inp = np.vstack([v[0].data.numpy() for v in validation])\n",
    "    save_val_tar = np.vstack([v[1].data.numpy() for v in validation])\n",
    "    save_pred_inp = np.vstack([p[0].data.numpy() for p in prediction])\n",
    "    save_pred_tar = np.vstack([p[1].data.numpy() for p in prediction])\n",
    "    np.savez_compressed(\n",
    "            save_path,\n",
    "            train_inp_seq=save_train_inp,\n",
    "            train_tar_seq=save_train_tar,\n",
    "            val_inp_seq=save_val_inp,\n",
    "            val_tar_seq=save_val_tar,\n",
    "            pred_inp_seq=save_pred_inp,\n",
    "            pred_tar_seq=save_pred_tar,\n",
    "        )\n",
    "\n",
    "\n",
    "# paths and other static stuff\n",
    "ROOT_DIR = Path.cwd()\n",
    "full_data_url = r'ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.xml.gz'\n",
    "glyc_only_data_url = r'https://www.uniprot.org/uniprot/?query=annotation:(type:carbohyd)&fil=reviewed%3Ayes&format=xml&compress=yes'\n",
    "\n",
    "datapath = ROOT_DIR / 'data'\n",
    "datapath.mkdir(exist_ok=True)\n",
    "\n",
    "full_data_path = ROOT_DIR / 'data' / 'uniprot_sprot.xml.gz'\n",
    "glyc_only_data_path = ROOT_DIR / 'data' / 'glyconly_uniprot_sprot.xml.gz'\n",
    "\n",
    "tokenized_data_path = ROOT_DIR / 'data' / f'tokenized_seq_data.npz'\n",
    "tokenized_n_glyc_path = ROOT_DIR / 'data' / f'n_glyc_data.npz'\n",
    "tokenized_o_glyc_path = ROOT_DIR / 'data' / f'o_glyc_data.npz'\n",
    "\n",
    "bert_embedded_n_glyc_path = ROOT_DIR / 'data' / f'bert_embedded_n_glyc_data.npz'\n",
    "bert_embedded_o_glyc_path = ROOT_DIR / 'data' / f'bert_embedded_o_glyc_data.npz'\n",
    "\n",
    "loss_data_path = ROOT_DIR / 'data' / 'loss_data.npz'\n",
    "checkpoint_path = ROOT_DIR / 'data' / 'model_checkpoint.pt'\n",
    "\n",
    "glyc_type = \"n\"\n",
    "batch_size = 1\n",
    "max_sequence_length = 2000  # we set a max sequence length to avoid inflation of tensors by rare large proteins\n",
    "\n",
    "# Get Swiss-Prot data if not already downloaded\n",
    "if not full_data_path.is_file():\n",
    "    print(\"Downloading {}\".format(full_data_url))\n",
    "    with closing(request.urlopen(full_data_url)) as r:\n",
    "        with open(full_data_path, 'wb') as f:\n",
    "            shutil.copyfileobj(r, f)\n",
    "\n",
    "# glycosylated proteins\n",
    "if not glyc_only_data_path.is_file():\n",
    "    print(\"Downloading {}\".format(glyc_only_data_url))\n",
    "    with closing(request.urlopen(glyc_only_data_url)) as r:\n",
    "        with open(glyc_only_data_path, 'wb') as f:\n",
    "            shutil.copyfileobj(r, f)\n",
    "\n",
    "# if data is not tokenized, tokenize data\n",
    "if not tokenized_data_path.is_file():\n",
    "    print(\"Tokenizing dataset from {}\".format(full_data_path))\n",
    "    \n",
    "    dataset = load_unencoded_data(full_data_path)\n",
    "    inp_n, inp_o, inp_non, tar_n, tar_o, tar_non  = tokenize_dataset(\n",
    "        dataset, \n",
    "        max_length=max_sequence_length\n",
    "    )  # we filter very long proteins which inflate tensor size\n",
    "    \n",
    "    inp_n = pad_sequences(inp_n, max_length=max_sequence_length)\n",
    "    tar_n = pad_sequences(tar_n, max_length=max_sequence_length)\n",
    "    inp_o = pad_sequences(inp_o, max_length=max_sequence_length)\n",
    "    tar_o = pad_sequences(tar_o, max_length=max_sequence_length)\n",
    "    inp_non = pad_sequences(inp_non, max_length=max_sequence_length)\n",
    "    tar_non = pad_sequences(tar_non, max_length=max_sequence_length)\n",
    "    \n",
    "    np.savez_compressed(\n",
    "        tokenized_data_path,\n",
    "        inp_seq_n=inp_n,\n",
    "        tar_seq_n=tar_n,\n",
    "        inp_seq_o=inp_o,\n",
    "        tar_seq_o=tar_o,\n",
    "        inp_seq_non=inp_non,\n",
    "        tar_seq_non=tar_non\n",
    "    )\n",
    "    \n",
    "    data_load = np.load(tokenized_data_path, allow_pickle=True)\n",
    "    \n",
    "    # glycosyaled protein data\n",
    "    inp_o = data_load['inp_seq_o']\n",
    "    tar_o = data_load['tar_seq_o']\n",
    "    \n",
    "    inp_n = data_load['inp_seq_n']\n",
    "    tar_n = data_load['tar_seq_n']\n",
    "    inp_n = inp_n[0:3000]  # subsample n-glyc dataset as it is too large to handle\n",
    "    tar_n = tar_n[0:3000]\n",
    "    \n",
    "    # non glycosyaled protein data\n",
    "    inp_non = data_load['inp_seq_non']\n",
    "    tar_non = data_load['tar_seq_non']\n",
    "    \n",
    "    inp_o, tar_o= concat_data(inp_o, tar_o, inp_non, tar_non)\n",
    "    inp_n, tar_n = concat_data(inp_n, tar_n, inp_non, tar_non)\n",
    "    \n",
    "\n",
    "# if data is already encoded, simply load data\n",
    "else:\n",
    "    print(\"Loading tokenized dataset {}\".format(tokenized_data_path))\n",
    "    data_load = np.load(tokenized_data_path, allow_pickle=True)\n",
    "    \n",
    "    # glycosyaled protein data\n",
    "    inp_o = data_load['inp_seq_o']\n",
    "    tar_o = data_load['tar_seq_o']\n",
    "    \n",
    "    inp_n = data_load['inp_seq_n']\n",
    "    tar_n = data_load['tar_seq_n']\n",
    "    inp_n = inp_n[0:3000]  # subsample n-glyc dataset as it is too large to handle\n",
    "    tar_n = tar_n[0:3000]\n",
    "    \n",
    "    # non glycosyaled protein data\n",
    "    inp_non = data_load['inp_seq_non']\n",
    "    tar_non = data_load['tar_seq_non']\n",
    "    \n",
    "    # concatenating glyco and non glyco protein data\n",
    "    inp_o, tar_o = concat_data(inp_o, tar_o, inp_non, tar_non)\n",
    "    inp_n, tar_n = concat_data(inp_n, tar_n, inp_non, tar_non)\n",
    "    \n",
    "#save training, validation and prediction datasets\n",
    "if not tokenized_n_glyc_path.is_file():\n",
    "    save_tokenized_data(inp_n, tar_n, tokenized_n_glyc_path)  \n",
    "if not tokenized_o_glyc_path.is_file():\n",
    "    save_tokenized_data(inp_o, tar_o, tokenized_o_glyc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT PROTEIN EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Custom BERT BOY ###\n",
    "class ProteinBERTBoy(nn.Module):\n",
    "    \"\"\"Example: Loading some linear layers on top of the pretrained Bert model\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ProteinBERTBoy, self).__init__()\n",
    "        self.bertbase = ProteinBertModel.from_pretrained('bert-base')\n",
    "        self.bertbase.max_length_embedding = max_sequence_length\n",
    "    \n",
    "    def forward(self, source_seq):\n",
    "        BERT_encoded_protein, _ = self.bertbase(source_seq)\n",
    "        return BERT_encoded_protein\n",
    "\n",
    "\n",
    "def BertEncode(BertModel, loader, number_of_proteins):\n",
    "    \"\"\"\n",
    "    Use pretrained protein BERT model to encode tokenized protein data.\n",
    "    set number_of_proteins to None to encode all proteins.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    labels = list()\n",
    "    start_stop_tokens = {5, 6}\n",
    "    encoded_proteins = list()\n",
    "    protein_lengths = list()\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for src, tgt in loader:\n",
    "            if count == number_of_proteins:\n",
    "                break\n",
    "            print(f\"Bert encoding protein: {count}\")\n",
    "            encoded_protein = BertModel(src)\n",
    "            encoded_protein = encoded_protein.squeeze()\n",
    "            encoded_protein = encoded_protein.numpy()\n",
    "            label = tgt.squeeze()\n",
    "            label = label.numpy()\n",
    "\n",
    "            store_index = []\n",
    "            for i in range(max_sequence_length):\n",
    "                check = label[i]\n",
    "                if check in start_stop_tokens:  # store start and stop token indices\n",
    "                    store_index.append(i)\n",
    "            protein_lengths.append(len(label[store_index]))\n",
    "            \n",
    "            encoded_proteins.append(encoded_protein)\n",
    "            labels.append(label)\n",
    "            count += 1            \n",
    "    return labels, encoded_proteins, protein_lengths\n",
    "\n",
    "\n",
    "def bert_encode_dataset(data_load_path, save_path):\n",
    "    \"\"\"encodes a tokenized dataset with bert\"\"\"\n",
    "    data_load = np.load(data_load_path, allow_pickle=True)\n",
    "    \n",
    "    train = Dataset(torch.from_numpy(data_load['train_inp_seq']), torch.from_numpy(data_load['train_tar_seq']))\n",
    "    train_loader = data.DataLoader(train, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    val = Dataset(torch.from_numpy(data_load['val_inp_seq']), torch.from_numpy(data_load['val_tar_seq']))\n",
    "    val_loader = data.DataLoader(val, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    pred = Dataset(torch.from_numpy(data_load['pred_inp_seq']), torch.from_numpy(data_load['pred_tar_seq']))\n",
    "    pred_loader = data.DataLoader(pred, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(\"BERT encoding train dataset\")\n",
    "    train_labels, train_embedded_proteins, train_protein_lengths = BertEncode(BertModel_instance, train_loader, None)\n",
    "    print(\"BERT encoding validation dataset\")\n",
    "    val_labels, val_embedded_proteins, val_protein_lengths = BertEncode(BertModel_instance, val_loader, None)\n",
    "    print(\"BERT encoding prediction dataset\")\n",
    "    pred_labels, pred_embedded_proteins, pred_protein_lengths = BertEncode(BertModel_instance, pred_loader, None)\n",
    "    \n",
    "    print(\"Saving BERT embedded dataset in train, val and prediction partitions, to {}\".format(save_path))\n",
    "    np.savez_compressed(\n",
    "        save_path,\n",
    "        train_inp_seq=train_embedded_proteins,\n",
    "        train_tar_seq=train_labels,\n",
    "        train_protein_lengths = train_protein_lengths,\n",
    "        val_inp_seq=val_embedded_proteins,\n",
    "        val_tar_seq=val_labels,\n",
    "        val_protein_lengths = val_protein_lengths,\n",
    "        pred_inp_seq=pred_embedded_proteins,\n",
    "        pred_tar_seq=pred_labels,\n",
    "        pred_protein_lengths = pred_protein_lengths\n",
    "    )\n",
    "    return (\n",
    "        train_labels, \n",
    "        train_embedded_proteins, \n",
    "        train_protein_lengths, \n",
    "        val_labels, \n",
    "        val_embedded_proteins, \n",
    "        val_protein_lengths, \n",
    "        pred_labels, \n",
    "        pred_embedded_proteins, \n",
    "        pred_protein_lengths\n",
    "    )\n",
    "\n",
    "# freezing the parameters of BERT\n",
    "BertModel_instance = ProteinBERTBoy()\n",
    "for param in BertModel_instance.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# saving BERT embedded dataset\n",
    "if not bert_embedded_n_glyc_path.is_file():\n",
    "    encoded_n_data = bert_encode_dataset(tokenized_n_glyc_path, bert_embedded_n_glyc_path)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "if not bert_embedded_o_glyc_path.is_file():\n",
    "    encoded_o_data = bert_encode_dataset(tokenized_o_glyc_path, bert_embedded_o_glyc_path)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downsampling training and validation datasets (only used in BERT + FFN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_o_data =  \"foo\"\n",
    "encoded_n_data =  \"foo\"\n",
    "def downsampling(labels, encoded_proteins):\n",
    "    \"\"\"\n",
    "    Downsampling the dataset to have equally many \n",
    "    glycosylated and non-glycosylated positions.\n",
    "    \"\"\"\n",
    "    glycosylated_positions = 0\n",
    "    non_glycosylated_positions = 0\n",
    "    glyco_encodings = list()\n",
    "    non_glyco_encodings = list()\n",
    "    labels = labels.flatten()\n",
    "    encoded_positions = encoded_proteins.reshape(-1, 768) \n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        encoded_position = encoded_positions[i]\n",
    "        if labels[i] == 6:\n",
    "            glyco_encodings.append(encoded_position)\n",
    "            glycosylated_positions += 1\n",
    "        elif labels[i] == 5:\n",
    "            non_glyco_encodings.append(encoded_position)\n",
    "            non_glycosylated_positions += 1\n",
    "        else:\n",
    "            #Ignore padded positions\n",
    "            pass\n",
    "            \n",
    "    non_glyco_encodings = np.asarray(non_glyco_encodings)\n",
    "    rand_idx = np.random.permutation(len(non_glyco_encodings))[0:glycosylated_positions]\n",
    "    dsampled_non_glyco_encodings = non_glyco_encodings[rand_idx] \n",
    "    \n",
    "    glyco_encodings = np.asarray(glyco_encodings)\n",
    "\n",
    "    dsampled_encodings = np.concatenate([dsampled_non_glyco_encodings, glyco_encodings])\n",
    "    dsampled_labels = np.concatenate([np.zeros(glycosylated_positions, dtype=int),\n",
    "                                             np.ones(glycosylated_positions, dtype=int)])\n",
    "    \n",
    "    return dsampled_encodings, dsampled_labels\n",
    "\n",
    "def downsample_sequences(bert_encoded_data_path, save_path):\n",
    "    \"\"\"\n",
    "    Run downsampling on train and validation partitions.\n",
    "    \"\"\"\n",
    "    if bert_encoded_data_path.is_file():\n",
    "        try:\n",
    "            data_load = np.load(bert_encoded_data_path, allow_pickle=True)\n",
    "            \n",
    "            train_seqs = data_load['train_inp_seq'] \n",
    "            train_labels = data_load['train_tar_seq']\n",
    "            \n",
    "            val_seqs = data_load['val_inp_seq']\n",
    "            val_labels = data_load['val_tar_seq']\n",
    "\n",
    "        except KeyError:\n",
    "            sys.exit(\"Object from data file could not be loaded. Please ensure that you are loading BERT embedded data file generated\\\n",
    "            from above cells.\")\n",
    "\n",
    "    train_seq_pos_dsampled, train_label_pos_dsampled = downsampling(train_labels, train_seqs)\n",
    "    val_seq_pos_dsampled, val_label_pos_dsampled = downsampling(val_labels, val_seqs)\n",
    "   \n",
    "    save_train_inp = train_seq_pos_dsampled\n",
    "    save_train_tar = train_label_pos_dsampled\n",
    "    save_val_inp = val_seq_pos_dsampled\n",
    "    save_val_tar = val_label_pos_dsampled\n",
    "   \n",
    "    np.savez_compressed(\n",
    "            save_path,\n",
    "            train_inp_seq=save_train_inp,\n",
    "            train_tar_seq=save_train_tar,\n",
    "            val_inp_seq=save_val_inp,\n",
    "            val_tar_seq=save_val_tar\n",
    "        )\n",
    "    return (\n",
    "        train_seq_pos_dsampled, \n",
    "        train_label_pos_dsampled, \n",
    "        val_seq_pos_dsampled, \n",
    "        val_label_pos_dsampled \n",
    "    )\n",
    "\n",
    "downsampled_encoded_glyco_position_data_path_n = datapath /  'dsampled_BERT_encoded_n_glyc_data_t.npz' \n",
    "downsampled_encoded_glyco_position_data_path_o = datapath /  'dsampled_BERT_encoded_o_glyc_data_t.npz'\n",
    "\n",
    "if not downsampled_encoded_glyco_position_data_path_n.is_file():\n",
    "    print(\"Downsampling N-glycosylated dataset\")\n",
    "    downsampled_n_data = downsample_sequences(bert_embedded_n_glyc_path, downsampled_encoded_glyco_position_data_path_n)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "if not downsampled_encoded_glyco_position_data_path_o.is_file():\n",
    "    print(\"Downsampling O-glycosylated dataset\")\n",
    "    downsampled_o_data = downsample_sequences(bert_embedded_o_glyc_path, downsampled_encoded_glyco_position_data_path_o)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot/tokenized Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_to_one_hot(data_load_path, save_path):\n",
    "    data_load = np.load(data_load_path, allow_pickle=True)\n",
    "    \n",
    "    train_inp, train_tar = torch.from_numpy(data_load['train_inp_seq']), torch.from_numpy(data_load['train_tar_seq'])\n",
    "    val_inp, val_tar = torch.from_numpy(data_load['val_inp_seq']), torch.from_numpy(data_load['val_tar_seq'])\n",
    "    pred_inp, pred_tar = torch.from_numpy(data_load['pred_inp_seq']), torch.from_numpy(data_load['pred_tar_seq'])\n",
    "\n",
    "    train_inp = torch.nn.functional.one_hot(train_inp, num_classes=30)\n",
    "    val_inp = torch.nn.functional.one_hot(val_inp, num_classes=30)\n",
    "    pred_inp = torch.nn.functional.one_hot(pred_inp, num_classes=30)\n",
    "    \n",
    "    np.savez_compressed(\n",
    "        save_path,\n",
    "        train_inp_seq=train_inp,\n",
    "        train_tar_seq=train_tar,\n",
    "        val_inp_seq=val_inp,\n",
    "        val_tar_seq=val_tar,\n",
    "        pred_inp_seq=pred_inp,\n",
    "        pred_tar_seq=pred_tar,\n",
    "    )\n",
    "    \n",
    "one_hot_path_o = datapath / 'one_hot_o_glyc_data.npz'\n",
    "one_hot_path_n = datapath / 'one_hot_n_glyc_data.npz'\n",
    "\n",
    "if not one_hot_path_o.is_file():\n",
    "    tokenized_to_one_hot(tokenized_o_glyc_path, one_hot_path_o)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "if not one_hot_path_n.is_file():\n",
    "    tokenized_to_one_hot(tokenized_n_glyc_path, one_hot_path_n)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models: Training and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_token_dataset_to_original(dataset):\n",
    "    \"\"\"writes the test parition data to fasta and txt file to enable use in other models\"\"\"\n",
    "    \n",
    "    inp_tokenizer = TAPETokenizer(vocab='iupac')\n",
    "    tar_tokenizer = TAPETokenizer(vocab='glycolysation')\n",
    "    seqs = list()\n",
    "    tars = list()\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        as_protein = inp_tokenizer.convert_ids_to_tokens(dataset.inputs[i].data.numpy())\n",
    "        seq = \"\".join([x for x in as_protein if x not in (\"<pad>\", \"<mask>\", \"<cls>\", \"<sep>\", \"<unk>\")])\n",
    "        seqs.append(seq)\n",
    "\n",
    "        as_one_hot = tar_tokenizer.convert_ids_to_tokens(dataset.labels[i].data.numpy())\n",
    "        special_tokens = (\"<pad>\", \"<mask>\", \"<cls>\", \"<sep>\", \"<unk>\")\n",
    "        as_one_hot = np.array([x for x in as_one_hot if x not in special_tokens], dtype=int)\n",
    "        tars.append(as_one_hot)\n",
    "        \n",
    "    return seqs, tars\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, train_loader, val_loader, epochs=20):\n",
    "    \"\"\"Function for training model\"\"\"\n",
    "    # training loop\n",
    "    best_loss = 10000000 #Just needs to be a high number \n",
    "    train_loss = list()\n",
    "    val_loss = list()\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch:{epoch + 1}/{epochs}')\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                loader = train_loader\n",
    "                dataset_size = t_dataset_size\n",
    "            else:\n",
    "                model.eval()\n",
    "                loader = val_loader\n",
    "                dataset_size = v_dataset_size\n",
    "            cur_loss = 0\n",
    "            running_acc = 0\n",
    "\n",
    "            for x, y in loader:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                # reset parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    output = model(x)\n",
    "                    _, preds = torch.max(output, 1)\n",
    "                    loss = criterion(output, y)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                #stats\n",
    "                cur_loss += loss.item()\n",
    "                running_acc += torch.sum(preds == y)\n",
    "\n",
    "            epoch_loss = cur_loss / dataset_size\n",
    "            epoch_acc = running_acc / dataset_size\n",
    "            \n",
    "            if phase == 'train':\n",
    "                train_loss.append(epoch_loss)\n",
    "                scheduler.step()\n",
    "            elif phase == 'val':\n",
    "                val_loss.append(epoch_loss)\n",
    "            \n",
    "            #save best model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_weights = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "            #print stats\n",
    "            print(f'{phase} loss: {epoch_loss} accuracy: {epoch_acc}')\n",
    "            \n",
    "    model.load_state_dict(best_weights)\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_glycosylation_from_embedded_FFN(dataset, positions_to_predict, the_model):\n",
    "    \"\"\"Reads embedded PROTEIN BERT dataset and makes glycosylation prediciton on positions. \n",
    "    Returns np array of probability assigned to glycosylation, target, predictions and probability assigned to correct class.\"\"\"\n",
    "    \n",
    "    pred_loader = data.DataLoader(prediction_dataset, batch_size=1, shuffle=False)\n",
    "    the_model.eval()\n",
    "    \n",
    "    count = 0\n",
    "    probs_for_true_class = list()\n",
    "    probs_for_glyc_only = list() #Needs this for ROC and AUC curve\n",
    "    predictions = list() #Need this for confusion matrix\n",
    "    targets = list() \n",
    "    softmax_function = nn.Softmax(dim=1)\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for AA, tgt in pred_loader:\n",
    "\n",
    "            if count == positions_to_predict:\n",
    "                break \n",
    "\n",
    "            # No glycosyaltion\n",
    "            if tgt.item() == 5:\n",
    "                target_index = 0\n",
    "                targets.append(target_index)\n",
    "            # Glycosylation\n",
    "            elif tgt.item() == 6:\n",
    "                target_index = 1\n",
    "                targets.append(target_index)\n",
    "            else:\n",
    "                #Ignore padding\n",
    "                target_index = \"padding\"\n",
    "\n",
    "            if target_index != \"padding\":\n",
    "                output = the_model(AA)\n",
    "                _, preds = torch.max(output, 1)\n",
    "                predictions.append(preds)\n",
    "                probs = softmax_function(output).squeeze()\n",
    "                probs_for_true_class.append(probs[target_index].item())\n",
    "                probs_for_glyc_only.append(probs[1].item())\n",
    "\n",
    "            count += 1\n",
    "            \n",
    "        output_predictions = np.vstack([np.asarray(probs_for_glyc_only), np.asarray(targets, dtype=int),\n",
    "                                       np.asarray(predictions), np.asarray(probs_for_true_class)])\n",
    "        \n",
    "    return output_predictions\n",
    "\n",
    "def train_model_transformer(model, criterion, optimizer, scheduler, train_loader, val_loader, pad_index, epochs=20):\n",
    "    \"\"\"Function for training transformer model\"\"\"\n",
    "    # training loop\n",
    "    best_loss = 10000000 #Just needs to be a high number \n",
    "    train_loss = list()\n",
    "    val_loss = list()\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch:{epoch + 1}/{epochs}')\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                loader = train_loader\n",
    "            else:\n",
    "                model.eval()\n",
    "                loader = val_loader\n",
    "            cur_loss = 0\n",
    "            running_acc = 0\n",
    "            n_batches = 0\n",
    "\n",
    "            for x, y in loader:\n",
    "                n_batches += 1\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                # reset parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    output = model(x[:, 1:], y[:, :-1])\n",
    "\n",
    "                    loss = criterion(output.transpose(1, 2), y[:, 1:])\n",
    "\n",
    "                    pad_mask = y[:, 1:] != pad_index\n",
    "                    preds = output.argmax(2)[pad_mask]\n",
    "                    acc = (preds == y[:, 1:][pad_mask]).sum().to(dtype=torch.float) / preds.shape[0]\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                #stats\n",
    "                cur_loss += loss.item()\n",
    "                running_acc += acc.item()\n",
    "\n",
    "            epoch_loss = cur_loss / n_batches\n",
    "            epoch_acc = running_acc / n_batches\n",
    "            \n",
    "            if phase == 'train':\n",
    "                train_loss.append(epoch_loss)\n",
    "                scheduler.step(epoch_loss)\n",
    "            elif phase == 'val':\n",
    "                val_loss.append(epoch_loss)\n",
    "            \n",
    "            #save best model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_weights = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "            #print stats\n",
    "            print(f'{phase} loss: {epoch_loss} accuracy: {epoch_acc}')\n",
    "            \n",
    "    model.load_state_dict(best_weights)\n",
    "    return model\n",
    "\n",
    "def predict_glycosylation_from_transformer(prediction_dataset, model, device):\n",
    "    \"\"\"Reads dataset and makes glycosylation prediciton on positions. \n",
    "    Returns np array of probability assigned to glycosylation, target, predictions and probability assigned to correct class.\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    probs_for_true_class = list()\n",
    "    probs_for_glyc_only = list() #Needs this for ROC and AUC curve\n",
    "    predictions = list() #Need this for confusion matrix\n",
    "    targets = list() \n",
    "\n",
    "    start_label = 2\n",
    "    end_label = 3\n",
    "    negative_label = 5\n",
    "    positive_label = 6\n",
    "\n",
    "    for i, (seq, tgt) in enumerate(prediction_dataset):\n",
    "        print(f\"Sequence {i+1} / {len(prediction_dataset)}\")#, end='\\r')\n",
    "\n",
    "        # Remove start token & padding\n",
    "        end_indices = np.where(seq == end_label)[0]\n",
    "        if end_indices.shape[0] > 0:\n",
    "            end_index = end_indices[0]\n",
    "        else:\n",
    "            end_index = seq.shape[0]\n",
    "        seq = seq[1:end_index]\n",
    "        tgt = tgt[1:end_index]\n",
    "\n",
    "        tgt_labels = [1 if AA == positive_label else 0 for AA in tgt]\n",
    "        targets.extend(tgt_labels)\n",
    "\n",
    "        preds, probs = predict_sequence_glycosylation_transformer(model, seq, start_label, device)\n",
    "\n",
    "        pred_labels = [1 if AA == positive_label else 0 for AA in preds]\n",
    "        predictions.extend(pred_labels)\n",
    "\n",
    "        prob_positions = torch.arange(0, probs.shape[0])\n",
    "        probs_for_true_class.extend(probs[prob_positions, tgt].tolist())\n",
    "        probs_for_glyc_only.extend(probs[prob_positions, positive_label].tolist())\n",
    "\n",
    "    output_predictions = np.vstack([np.asarray(probs_for_glyc_only), np.asarray(targets, dtype=int),\n",
    "                                np.asarray(predictions, dtype=int), np.asarray(probs_for_true_class)])\n",
    "        \n",
    "    return output_predictions\n",
    "\n",
    "def predict_sequence_glycosylation_transformer(model, seq, start_label, device):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        preds = [start_label]\n",
    "        probs = []\n",
    "\n",
    "        seq = torch.tensor(seq, device=device).unsqueeze(0)\n",
    "\n",
    "        for _ in range(seq.shape[1]):\n",
    "            tgt = torch.tensor(preds, device=device).unsqueeze(0)\n",
    "\n",
    "            output = model(seq, tgt)\n",
    "\n",
    "            next_preds = output[0, -1, :]\n",
    "            next_label = next_preds.argmax()\n",
    "            next_probs = softmax(next_preds, dim=0)\n",
    "\n",
    "            preds.append(next_label.item())\n",
    "            probs.append(next_probs)\n",
    "\n",
    "    probs = torch.stack(probs)\n",
    "    return preds[1:], probs\n",
    "\n",
    "def cm_analysis(y_true, y_pred, labels, ymap=None, figsize=(10,10)):\n",
    "    \"\"\"\n",
    "    Generate matrix plot of confusion matrix with pretty annotations.\n",
    "    The plot image is saved to disk.\n",
    "    #https://gist.github.com/hitvoice/36cf44689065ca9b927431546381a3f7 steal!\n",
    "    args: \n",
    "      y_true:    true label of the data, with shape (nsamples,)\n",
    "      y_pred:    prediction of the data, with shape (nsamples,)\n",
    "      labels:    string array, name the order of class labels in the confusion matrix.\n",
    "                 use `clf.classes_` if using scikit-learn models.\n",
    "                 with shape (nclass,).\n",
    "      ymap:      dict: any -> string, length == nclass.\n",
    "                 if not None, map the labels & ys to more understandable strings.\n",
    "                 Caution: original y_true, y_pred and labels must align.\n",
    "      figsize:   the size of the figure plotted.\n",
    "    \"\"\"\n",
    "    if ymap is not None:\n",
    "        y_pred = [ymap[yi] for yi in y_pred]\n",
    "        y_true = [ymap[yi] for yi in y_true]\n",
    "        labels = [ymap[yi] for yi in labels]\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    cm_perc = cm / cm_sum.astype(float) * 100\n",
    "    annot = np.empty_like(cm).astype(str)\n",
    "    nrows, ncols = cm.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            c = cm[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            if i == j:\n",
    "                s = cm_sum[i]\n",
    "#                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
    "                annot[i, j] = f'{round(p,2)}%\\n{c}'\n",
    "            elif c == 0:\n",
    "                annot[i, j] = ''\n",
    "            else:\n",
    "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
    "    cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "    cm.index.name = 'True'\n",
    "    cm.columns.name = 'Predicted'\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    sns.heatmap(cm, annot=annot, fmt='', ax=ax, cmap='OrRd', linewidths = 1.5, linecolor='black')\n",
    "    plt.savefig(\"confusion_matrix\", dpi=800)\n",
    "\n",
    "\n",
    "def predict_glycosylation(prob_glyc, threshold = 0.5):\n",
    "    \"\"\"Needed this function for LSTM predictions, where the data was probability for glycosylation\"\"\"\n",
    "    predictions = list()\n",
    "    for prob in prob_glyc:\n",
    "        if prob >= threshold:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "            \n",
    "    return predictions\n",
    "\n",
    "\n",
    "def parse_output_file(path, new_protein_regex, glyc_pred_regex, no_match_string):\n",
    "    \"\"\"Parses the output file of netN or netO glyc tools and transforms them into a usable format.\"\"\"\n",
    "    net_preds = list()\n",
    "    tar_seq = None\n",
    "    parse_flag = False\n",
    "    with open(path, \"r\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            new_protein = re.search(new_protein_regex, line)\n",
    "            if new_protein is not None:\n",
    "                length = int(new_protein.group(1))\n",
    "                tar_seq = np.zeros(length)\n",
    "            glyc_pred = re.search(glyc_pred_regex, line)\n",
    "            if glyc_pred is not None:\n",
    "                position = int(glyc_pred.group(1)) - 1\n",
    "                score = float(glyc_pred.group(2))\n",
    "                tar_seq[position] = score\n",
    "                parse_flag = True\n",
    "            if line[0:4] == \"----\" and parse_flag is True:\n",
    "                net_preds.append(tar_seq)\n",
    "                tar_seq = None\n",
    "                parse_flag = False\n",
    "            elif line == no_match_string:\n",
    "                net_preds.append(tar_seq)\n",
    "                tar_seq = None\n",
    "                parse_flag = False\n",
    "    return np.array(net_preds, dtype=object)    \n",
    "    \n",
    "    \n",
    "# paths and other static stuff\n",
    "ROOT_DIR = Path.cwd()\n",
    "datapath = ROOT_DIR / 'data'\n",
    "full_data_path = ROOT_DIR / 'data' / 'uniprot_sprot.xml.gz'\n",
    "glyc_only_data_path = ROOT_DIR / 'data' / 'glyconly_uniprot_sprot.xml.gz'\n",
    "\n",
    "encoded_data_path = ROOT_DIR / 'data' / f'encode_and_decode_seq_data.npz'\n",
    "encoded_n_glyc_path = ROOT_DIR / 'data' / f'n_glyc_data.npz'\n",
    "encoded_o_glyc_path = ROOT_DIR / 'data' / f'o_glyc_data.npz'\n",
    "bert_embedded_n_glyc_path = ROOT_DIR / 'data' / f'bert_embedded_n_glyc_data.npz'\n",
    "bert_embedded_o_glyc_path = ROOT_DIR / 'data' / f'bert_embedded_o_glyc_data.npz'\n",
    "\n",
    "loss_data_path = ROOT_DIR / 'data' / 'loss_data.npz'\n",
    "checkpoint_path = ROOT_DIR / 'data' / 'model_checkpoint.pt'\n",
    "\n",
    "model_save_dir = ROOT_DIR / 'models'\n",
    "model_save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "#device = torch.device(\"cuda\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer O glyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size, dropout=0.1, max_seq_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_seq_len, embedding_size)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_size, 2).float() * (-math.log(10000.0) / embedding_size))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.shape[0], :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class BERTEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BERTEncoder, self).__init__()\n",
    "\n",
    "        self.bert = tape.ProteinBertModel.from_pretrained('bert-base')\n",
    "\n",
    "    def freeze_encoder(self):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src: [batch_size, seq_length]\n",
    "\n",
    "        mem_embed, _ = self.bert(src)\n",
    "\n",
    "        # [batch_size, seq_length, embed_size]\n",
    "        return mem_embed\n",
    "\n",
    "\n",
    "class TransformerBERTDecoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        trg_vocab_size,\n",
    "        num_heads,\n",
    "        num_layers,\n",
    "        dim_feedforward,\n",
    "        dropout,\n",
    "        trg_pad_idx,\n",
    "        max_seq_len,\n",
    "        device,\n",
    "        bert_embedding_size=768\n",
    "    ):\n",
    "        super(TransformerBERTDecoder, self).__init__()\n",
    "\n",
    "        self.bert_embedding_size = bert_embedding_size\n",
    "        self.sqrt_embedding_size = math.sqrt(self.bert_embedding_size)\n",
    "        self.dropout = dropout\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "        self.trg_word_embedding = nn.Embedding(trg_vocab_size, self.bert_embedding_size)\n",
    "        self.trg_position_embedding = PositionalEncoding(\n",
    "            self.bert_embedding_size, self.dropout, max_seq_len\n",
    "        )\n",
    "\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=self.bert_embedding_size, nhead=num_heads,\n",
    "            dim_feedforward=dim_feedforward, dropout=self.dropout\n",
    "        )\n",
    "\n",
    "        self.decoder_norm = nn.LayerNorm(normalized_shape=self.bert_embedding_size)\n",
    "\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            decoder_layer=self.decoder_layer, num_layers=num_layers, norm=self.decoder_norm\n",
    "        )\n",
    "        \n",
    "        self.out_fc = nn.Linear(\n",
    "            in_features=self.bert_embedding_size,\n",
    "            out_features=trg_vocab_size\n",
    "        )\n",
    "\n",
    "        self.initialize_decoder()\n",
    "\n",
    "    def initialize_decoder(self):\n",
    "        for param in self.decoder.parameters():\n",
    "            if param.dim() > 1:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "            mask = torch.triu(torch.ones((sz, sz), device=self.device)).transpose(0, 1)\n",
    "            mask = mask.masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, 0)\n",
    "            return mask\n",
    "\n",
    "    def change_max_seq_len(self, max_seq_len):\n",
    "        self.trg_position_embedding = PositionalEncoding(\n",
    "            self.bert_embedding_size, self.dropout, max_seq_len\n",
    "        ).to(self.device)\n",
    "\n",
    "    def forward(self, mem_embed, trg):\n",
    "        # mem_embed: [batch_size, seq_length, embed_size]\n",
    "        # trg: [batch_size, seq_length]\n",
    "\n",
    "        trg_embed = self.trg_word_embedding(trg)\n",
    "\n",
    "        trg_embed = trg_embed * self.sqrt_embedding_size\n",
    "\n",
    "        trg_embed = self.trg_position_embedding(trg_embed)\n",
    "\n",
    "        trg_padding_mask = (trg == self.trg_pad_idx) # [batch_size, seq_length]\n",
    "\n",
    "        trg_mask = self.generate_square_subsequent_mask(trg.shape[1]) # [seq_length, seq_length]\n",
    "\n",
    "        out = self.decoder(\n",
    "            trg_embed.transpose(0, 1),\n",
    "            mem_embed.transpose(0, 1),\n",
    "            tgt_mask=trg_mask,\n",
    "            tgt_key_padding_mask=trg_padding_mask\n",
    "        ).transpose(0, 1) # [batch_size, seq_length, embed_size]\n",
    "\n",
    "        # [batch_size, seq_length, out_classes]\n",
    "        return self.out_fc(out)\n",
    "\n",
    "\n",
    "class TransformerBERTEncoderDecoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        trg_vocab_size,\n",
    "        num_heads,\n",
    "        num_layers,\n",
    "        dim_feedforward,\n",
    "        dropout,\n",
    "        trg_pad_idx,\n",
    "        max_seq_len,\n",
    "        device,\n",
    "        bert_embedding_size=768\n",
    "    ):\n",
    "        super(TransformerBERTEncoderDecoder, self).__init__()\n",
    "\n",
    "        self.encoder = BERTEncoder()\n",
    "\n",
    "        self.decoder = TransformerBERTDecoder(\n",
    "            trg_vocab_size,\n",
    "            num_heads,\n",
    "            num_layers,\n",
    "            dim_feedforward,\n",
    "            dropout,\n",
    "            trg_pad_idx,\n",
    "            max_seq_len,\n",
    "            device,\n",
    "            bert_embedding_size\n",
    "        )\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        mem_embed = self.encoder(src)\n",
    "        out = self.decoder(mem_embed, trg)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load data in dataloaders ###\n",
    "glyc_type = \"o\"\n",
    "batch_size = 6\n",
    "\n",
    "train_npz = np.load(datapath / \"o_embedded_train.npz\", allow_pickle=True)\n",
    "train = Dataset(torch.from_numpy(train_npz['train_inp_seq']), torch.from_numpy(train_npz['train_tar_seq']))\n",
    "train_loader = data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_npz = np.load(datapath / \"o_embedded_val.npz\", allow_pickle=True)\n",
    "val = Dataset(torch.from_numpy(val_npz['val_inp_seq']), torch.from_numpy(val_npz['val_tar_seq']))\n",
    "val_loader = data.DataLoader(val, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training ###\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs = 20\n",
    "learning_rate = 1e-6\n",
    "device = torch.device(\"cuda\")\n",
    "save_model = 'yes_please'\n",
    "timestamp = str(time.time())\n",
    "\n",
    "# Model hyperparameters\n",
    "num_heads = 8\n",
    "num_layers = 3\n",
    "dim_feedforward = 1024\n",
    "dropout = 0.1\n",
    "max_seq_len = 2500\n",
    "trg_vocab_size = 7 # GLYC_VOCAB\n",
    "trg_pad_idx = 0\n",
    "trg_positive_idx = 6\n",
    "\n",
    "model = TransformerBERTDecoder(\n",
    "#model = TransformerBERTEncoderDecoder(\n",
    "    trg_vocab_size=trg_vocab_size,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    dropout=dropout,\n",
    "    trg_pad_idx=trg_pad_idx,\n",
    "    max_seq_len=max_seq_len,\n",
    "    device=device\n",
    ").to(device)\n",
    "#model.encoder.freeze_encoder()\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, factor=0.1, patience=10, verbose=True\n",
    ")\n",
    "\n",
    "loss_weights = torch.ones((trg_vocab_size,), device=device)\n",
    "loss_weights[trg_positive_idx] = 100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx, weight=loss_weights).to(device)\n",
    "\n",
    "trained_model = train_model_transformer(model, criterion, optimizer, scheduler, train_loader, val_loader, pad_index=trg_pad_idx, epochs=num_epochs)\n",
    "if save_model == 'yes_please':\n",
    "    filename = 'Transformer_atop_BERT_model' + '_' + glyc_type + '-' + timestamp\n",
    "    torch.save(trained_model, model_save_dir / filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prediction ###\n",
    "device = torch.device(\"cuda\")\n",
    "pred_npz = np.load(datapath / \"o_embedded_pred.npz\", allow_pickle=True)\n",
    "cm_plot_labels = [\"No O glycosylation\", \"O glycosylation\"]\n",
    "save_pred_path = datapath / 'Transformer_o_glyc_predictions'\n",
    "pred = Dataset(torch.from_numpy(pred_npz['pred_inp_seq']), torch.from_numpy(pred_npz['pred_tar_seq']))\n",
    "\n",
    "trained_model.to(device)\n",
    "trained_model.decoder.device = device\n",
    "out = predict_glycosylation_from_transformer(pred, trained_model, device)\n",
    "pred=out[0] # Predicted glyc probability \n",
    "tar_seq=out[1] # Target label\n",
    "actual_pred=out[2] # Predicted label\n",
    "true_class_prob=out[3] # Predicted probability of target label\n",
    "np.savez_compressed(save_pred_path, pred=out[0], tar_seq=out[1], actual_pred=out[2], true_class_prob=out[3])\n",
    "cm_analysis(tar_seq, actual_pred, cm_plot_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer N-glyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load data in dataloaders ###\n",
    "glyc_type = \"n\"\n",
    "batch_size = 6\n",
    "\n",
    "train_npz = np.load(datapath / \"n_embedded_train.npz\", allow_pickle=True)\n",
    "train = Dataset(torch.from_numpy(train_npz['train_inp_seq']), torch.from_numpy(train_npz['train_tar_seq']))\n",
    "train_loader = data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_npz = np.load(datapath / \"n_embedded_val.npz\", allow_pickle=True)\n",
    "val = Dataset(torch.from_numpy(val_npz['val_inp_seq']), torch.from_numpy(val_npz['val_tar_seq']))\n",
    "val_loader = data.DataLoader(val, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training ###\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs = 20\n",
    "learning_rate = 1e-6\n",
    "device = torch.device(\"cuda\")\n",
    "save_model = 'yes_please'\n",
    "timestamp = str(time.time())\n",
    "\n",
    "# Model hyperparameters\n",
    "num_heads = 8\n",
    "num_layers = 3\n",
    "dim_feedforward = 1024\n",
    "dropout = 0.1\n",
    "max_seq_len = 2500\n",
    "trg_vocab_size = 7 # GLYC_VOCAB\n",
    "trg_pad_idx = 0\n",
    "trg_positive_idx = 6\n",
    "\n",
    "model = TransformerBERTDecoder(\n",
    "#model = TransformerBERTEncoderDecoder(\n",
    "    trg_vocab_size=trg_vocab_size,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    dropout=dropout,\n",
    "    trg_pad_idx=trg_pad_idx,\n",
    "    max_seq_len=max_seq_len,\n",
    "    device=device\n",
    ").to(device)\n",
    "#model.encoder.freeze_encoder()\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, factor=0.1, patience=10, verbose=True\n",
    ")\n",
    "\n",
    "loss_weights = torch.ones((trg_vocab_size,), device=device)\n",
    "loss_weights[trg_positive_idx] = 100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx, weight=loss_weights).to(device)\n",
    "\n",
    "trained_model = train_model_transformer(model, criterion, optimizer, scheduler, train_loader, val_loader, pad_index=trg_pad_idx, epochs=num_epochs)\n",
    "if save_model == 'yes_please':\n",
    "    filename = 'Transformer_atop_BERT_model' + '_' + glyc_type + '-' + timestamp\n",
    "    torch.save(trained_model, model_save_dir / filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prediction ###\n",
    "device = torch.device(\"cuda\")\n",
    "pred_npz = np.load(datapath / \"n_embedded_pred.npz\", allow_pickle=True)\n",
    "cm_plot_labels = [\"No N glycosylation\", \"N glycosylation\"]\n",
    "save_pred_path = datapath / 'Transformer_n_glyc_predictions'\n",
    "pred = Dataset(torch.from_numpy(pred_npz['pred_inp_seq']), torch.from_numpy(pred_npz['pred_tar_seq']))\n",
    "\n",
    "trained_model.to(device)\n",
    "trained_model.decoder.device = device\n",
    "out = predict_glycosylation_from_transformer(pred, trained_model, device)\n",
    "pred=out[0] # Predicted glyc probability \n",
    "tar_seq=out[1] # Target label\n",
    "actual_pred=out[2] # Predicted label\n",
    "true_class_prob=out[3] # Predicted probability of target label\n",
    "np.savez_compressed(save_pred_path, pred=out[0], tar_seq=out[1], actual_pred=out[2], true_class_prob=out[3])\n",
    "cm_analysis(tar_seq, actual_pred, cm_plot_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFN N-glyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FFN, self).__init__()\n",
    "        #First input: BERT embedding size\n",
    "        bert_position_embedding_size = 768 \n",
    "        self.ff_l1_size = 450\n",
    "        self.ff_l2_size = 300\n",
    "        self.ff_l3_size = 200\n",
    "        num_of_class = 2 #Gly no Gly\n",
    "        \n",
    "        self.ff = nn.Sequential(nn.Linear(bert_position_embedding_size, self.ff_l1_size),\n",
    "                                nn.ReLU(),\n",
    "                                BatchNorm1d(self.ff_l1_size),\n",
    "                                nn.Dropout(0.35),\n",
    "                                nn.Linear(self.ff_l1_size, self.ff_l2_size),\n",
    "                                nn.ReLU(),\n",
    "                                BatchNorm1d(self.ff_l2_size),\n",
    "                                nn.Dropout(0.35),\n",
    "                                nn.Linear(self.ff_l2_size, self.ff_l3_size),\n",
    "                                nn.ReLU(),\n",
    "                                BatchNorm1d(self.ff_l3_size),\n",
    "                                nn.Linear(self.ff_l3_size, num_of_class))      \n",
    "    def forward(self, x):\n",
    "        out = self.ff(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load data in dataloaders ###\n",
    "glyc_type = \"n\"\n",
    "bert_embedded_data_path = datapath / 'dsampled_BERT_encoded_n_glyc_data.npz'\n",
    "batch_size = 2\n",
    "data_load = np.load(bert_embedded_data_path, allow_pickle=True)\n",
    "train = Dataset(torch.from_numpy(data_load['train_inp_seq']), torch.from_numpy(data_load['train_tar_seq']).long())\n",
    "train_loader = data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "t_dataset_size = len(train)\n",
    "\n",
    "val = Dataset(torch.from_numpy(data_load['val_inp_seq']), torch.from_numpy(data_load['val_tar_seq']).long())\n",
    "val_loader = data.DataLoader(val, batch_size=batch_size, shuffle=True)\n",
    "v_dataset_size = len(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training ###\n",
    "ffn = FFN()\n",
    "ffn.cuda()\n",
    "device = torch.device(\"cuda\")\n",
    "timestamp = str(time.time())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "LEARNING_RATE = 0.001\n",
    "optimizer = optim.Adam(ffn.parameters(), lr=LEARNING_RATE)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "save_model = 'yes_please'\n",
    "trained_model = train_model(ffn, criterion, optimizer, exp_lr_scheduler, train_loader, val_loader, epochs=10)\n",
    "if save_model == 'yes_please':\n",
    "    filename = 'FFN_atop_BERT_model' + '_' + glyc_type + '-' + timestamp\n",
    "    torch.save(trained_model, model_save_dir / filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Predictions on prediction dataset ###\n",
    "data_load = np.load(datapath /  'bert_embedded_n_glyc_data.npz', allow_pickle=True)\n",
    "cm_plot_labels = [\"No N glycosyalation\", \"N glcyosylation\"]\n",
    "save_pred_path = datapath / 'FFN_n_glyc_predictions'\n",
    "trained_model.eval()\n",
    "trained_model.cpu()   \n",
    "pred_seqs = torch.from_numpy(data_load['pred_inp_seq']) \n",
    "pred_labels = torch.from_numpy( data_load['pred_tar_seq']).long()\n",
    "\n",
    "### FFN takes one AA at a time\n",
    "pred_seqs = pred_seqs.reshape(-1, 768) \n",
    "pred_labels = pred_labels.flatten()\n",
    "\n",
    "prediction_dataset = Dataset(pred_seqs, pred_labels)\n",
    "device = torch.device(\"cpu\")\n",
    "out = predict_glycosylation_from_embedded_FFN(prediction_dataset, None, trained_model)\n",
    "pred=out[0]\n",
    "tar_seq=out[1]\n",
    "actual_pred=out[2]\n",
    "true_class_prob=out[3]\n",
    "np.savez_compressed(save_pred_path, glyc_prob=out[0], tar_seq=out[1], actual_pred=out[2], true_class_prob=out[3])\n",
    "cm_analysis(tar_seq, actual_pred, cm_plot_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFN O-glyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load data in dataloaders ###\n",
    "glyc_type = \"o\"\n",
    "bert_embedded_data_path = datapath / 'dsampled_BERT_encoded_o_glyc_data.npz'\n",
    "batch_size = 2\n",
    "data_load = np.load(bert_embedded_data_path, allow_pickle=True)\n",
    "train = Dataset(torch.from_numpy(data_load['train_inp_seq']), torch.from_numpy(data_load['train_tar_seq']).long())\n",
    "train_loader = data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "t_dataset_size = len(train)\n",
    "val = Dataset(torch.from_numpy(data_load['val_inp_seq']), torch.from_numpy(data_load['val_tar_seq']).long())\n",
    "val_loader = data.DataLoader(val, batch_size=batch_size, shuffle=True)\n",
    "v_dataset_size = len(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training ###\n",
    "ffn = FFN()\n",
    "ffn.cuda()\n",
    "device = torch.device(\"cuda\")\n",
    "timestamp = str(time.time())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "LEARNING_RATE = 0.001\n",
    "optimizer = optim.Adam(ffn.parameters(), lr=LEARNING_RATE)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "save_model = 'yes_please'\n",
    "trained_model = train_model(ffn, criterion, optimizer, exp_lr_scheduler, train_loader, val_loader, epochs=10)\n",
    "if save_model == 'yes_please':\n",
    "    filename = 'FFN_atop_BERT_model' + '_' + glyc_type + '-' + timestamp\n",
    "    torch.save(trained_model, model_save_dir / filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prediction ###\n",
    "data_load = np.load(datapath /  'bert_embedded_o_glyc_data.npz', allow_pickle=True)\n",
    "cm_plot_labels = [\"No O glycosyalation\", \"O glcyosylation\"]\n",
    "save_pred_path = datapath / 'FFN_o_glyc_predictions'\n",
    "trained_model.eval()\n",
    "trained_model.cpu()\n",
    "pred_seqs = torch.from_numpy(data_load['pred_inp_seq']) \n",
    "pred_labels = torch.from_numpy( data_load['pred_tar_seq']).long() \n",
    "\n",
    "### FFN takes one AA at a time\n",
    "pred_seqs = pred_seqs.reshape(-1, 768) \n",
    "pred_labels = pred_labels.flatten()\n",
    "\n",
    "prediction_dataset = Dataset(pred_seqs, pred_labels)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "out = predict_glycosylation_from_embedded_FFN(prediction_dataset, None, trained_model)\n",
    "pred=out[0]\n",
    "tar_seq=out[1]\n",
    "actual_pred=out[2]\n",
    "true_class_prob=out[3]\n",
    "np.savez_compressed(save_pred_path, glyc_prob=out[0], tar_seq=out[1], actual_pred=out[2], true_class_prob=out[3])\n",
    "cm_analysis(tar_seq, actual_pred, cm_plot_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            hidden_size = 50,\n",
    "            src_vocab_size = 30,\n",
    "            tgt_vocab_size = 7\n",
    "    ):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        # Recurrent layer\n",
    "        self.lstm = nn.LSTM(input_size=src_vocab_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=3,\n",
    "                            bidirectional=False,\n",
    "                            dropout=0.1)\n",
    "\n",
    "        # Output layer\n",
    "        self.l_out = nn.Linear(in_features=hidden_size,\n",
    "                               out_features=tgt_vocab_size,\n",
    "                               bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # RNN returns output and last hidden state\n",
    "        x, (h, c) = self.lstm(x)\n",
    "        x = x.view(-1, self.lstm.hidden_size)\n",
    "        x = F.relu(x)\n",
    "        x = self.l_out(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "def train_lstm(model, optimizer, lr_sched, criterion, load_model, save_model, epochs, checkpoint_path, loss_data_path, device):\n",
    "    \"\"\"\n",
    "    Training loop for LSTM\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    train_loss = list()\n",
    "    valid_loss = list()\n",
    "    test_loss = list()\n",
    "\n",
    "    if load_model:\n",
    "        train_loss, valid_loss = load_checkpoint(model, optimizer, checkpoint_path, loss_data_path)\n",
    "\n",
    "    # training loop\n",
    "    print(\"Starting training for {} epochs\".format(epochs))\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        cur_train_loss = 0\n",
    "        cur_valid_loss = 0\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            #x = BertModel_instance(x)\n",
    "            x = x.permute(1, 0, 2).float().to(device)  # change dims to match LSTM module expected dim\n",
    "            y = y.long().to(device)\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y.view(-1))\n",
    "            cur_train_loss += loss.item()\n",
    "\n",
    "            # propegate gradient\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # Evaluate on a single batch, do not propagate gradients\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for x, y in val_loader:\n",
    "                #x = BertModel_instance(x)\n",
    "                x = x.permute(1, 0, 2).float().to(device)\n",
    "                y = y.long().to(device)\n",
    "                output = model(x)\n",
    "                loss = criterion(output, y.view(-1))\n",
    "                cur_valid_loss += loss.item()\n",
    "\n",
    "        # get mean losses\n",
    "        train_loss.append(cur_train_loss / len(train))\n",
    "        valid_loss.append(cur_valid_loss / len(val))\n",
    "\n",
    "        #if epoch % 1 == 0:\n",
    "        print(\"Epoch %2i : Train Loss %f, Validation Loss %f\" % (epoch+1, train_loss[-1], valid_loss[-1]))\n",
    "\n",
    "        if save_model:\n",
    "            save_checkpoint(model, optimizer, train_loss, valid_loss, checkpoint_path, loss_data_path)\n",
    "        lr_sched.step()\n",
    "\n",
    "        # save final model\n",
    "    torch.save({\"model_state\": model.state_dict()}, checkpoint_path)\n",
    "    \n",
    "\n",
    "def predict_lstm(model, checkpoint_path, test_loader, rnn_preds, rnn_preds_readable, device):\n",
    "    \"\"\"\n",
    "    Make predictions on a dataset with a trained LSTM model\n",
    "    \"\"\"\n",
    "    # load trained model\n",
    "    model_data = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(model_data[\"model_state\"])\n",
    "    model = model.to(device)\n",
    "\n",
    "    # make predictions on test set\n",
    "    tar_tokenizer = TAPETokenizer(vocab='glycolysation')\n",
    "    pred_array = list()\n",
    "    tar_array = list()\n",
    "    outfile = open(rnn_preds_readable, \"w\")\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for i, (x, y) in enumerate(test_loader):\n",
    "            x = x.permute(1, 0, 2).float().to(device)\n",
    "            output = model(x)\n",
    "\n",
    "            # convert to human readable format and print to outfile\n",
    "            output_max = F.softmax(output, dim=1).to(device).data.numpy()\n",
    "            output_max = np.array([np.argmax(x) for x in output_max])  # get most likely token\n",
    "            output_max = tar_tokenizer.convert_ids_to_tokens(output_max)\n",
    "            y = tar_tokenizer.convert_ids_to_tokens(y[0].to(\"cpu\").data.numpy())\n",
    "            print(\"Prediction:\", output_max, \"Target:   \",  y, sep=\"\\n\", file=outfile)\n",
    "            print(\"\\n\", file=outfile)\n",
    "\n",
    "            # slice away start tokens and paddings for performance evaluation \n",
    "            # as we don't care about predictions in padding regions\n",
    "            start_pos = y.index(\"<cls>\")\n",
    "            end_pos = y.index(\"<sep>\")\n",
    "            output_sliced = output[:, 5:]\n",
    "            output_sliced = F.softmax(output_sliced[1:end_pos], dim=1)\n",
    "            y_sliced = y[1:end_pos]\n",
    "            y_sliced = [int(x) for x in y_sliced]\n",
    "\n",
    "            pred_array.append(output_sliced[:, 1])\n",
    "            tar_array.append(y_sliced)\n",
    "    pred_array = np.hstack(pred_array)\n",
    "    tar_array = np.hstack(tar_array)\n",
    "    np.savez_compressed(datapath / rnn_preds, pred=pred_array, tar=tar_array)\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O-glyc with BERT encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "epochs = 50\n",
    "load_model = False  # set True to load a checkpoint before resuming training\n",
    "save_model = True  # set True to make a checkpoint of the model after each epoch\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "# load data\n",
    "data_load = np.load(datapath / \"bert_embedded_o_glyc_data.npz\", allow_pickle=True)\n",
    "\n",
    "train = Dataset(torch.from_numpy(data_load['train_inp_seq']), torch.from_numpy(data_load['train_tar_seq']).long())\n",
    "train_loader = data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val = Dataset(torch.from_numpy(data_load['val_inp_seq']), torch.from_numpy(data_load['val_tar_seq']).long())\n",
    "val_loader = data.DataLoader(val, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# set paths for model checkpoints\n",
    "checkpoint_path = datapath / 'model_rnn_o.pt'\n",
    "loss_data_path = datapath / 'loss_data_o.npz'\n",
    "\n",
    "# path for saving predictions\n",
    "rnn_preds = datapath / \"rnn_pred_o.npz\"\n",
    "rnn_preds_readable = datapath / \"rnn_preds_translated_o.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(src_vocab_size=768, tgt_vocab_size=7).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0)\n",
    "lr_sched = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_lstm(\n",
    "    model, \n",
    "    optimizer, \n",
    "    lr_sched, \n",
    "    criterion, \n",
    "    load_model, \n",
    "    save_model, \n",
    "    epochs,\n",
    "    checkpoint_path,\n",
    "    loss_data_path,\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test set\n",
    "test = Dataset(torch.from_numpy(data_load['pred_inp_seq']), torch.from_numpy(data_load['pred_tar_seq']).long())\n",
    "test_loader = data.DataLoader(test, batch_size=1, shuffle=False)\n",
    "model = LSTM(src_vocab_size=768, tgt_vocab_size=7).to(device)\n",
    "predict_lstm(model, checkpoint_path, test_loader, rnn_preds, rnn_preds_readable, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-glyc with BERT encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "#epochs = 50\n",
    "load_model = False  # set True to load a checkpoint before resuming training\n",
    "save_model = True  # set True to make a checkpoint of the model after each epoch\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "# load data\n",
    "data_load = np.load(datapath / \"bert_embedded_n_glyc_data.npz\", allow_pickle=True)\n",
    "\n",
    "train = Dataset(torch.from_numpy(data_load['train_inp_seq']), torch.from_numpy(data_load['train_tar_seq']).long())\n",
    "train_loader = data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val = Dataset(torch.from_numpy(data_load['val_inp_seq']), torch.from_numpy(data_load['val_tar_seq']).long())\n",
    "val_loader = data.DataLoader(val, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# set paths for model checkpoints\n",
    "checkpoint_path = datapath / 'model_rnn_n.pt'\n",
    "loss_data_path = datapath / 'loss_data_n.npz'\n",
    "\n",
    "# path for saving predictions\n",
    "rnn_preds = datapath / \"rnn_pred_n.npz\"\n",
    "rnn_preds_readable = datapath / \"rnn_preds_translated_n.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(src_vocab_size=768, tgt_vocab_size=7).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0)\n",
    "lr_sched = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_lstm(\n",
    "    model, \n",
    "    optimizer, \n",
    "    lr_sched, \n",
    "    criterion, \n",
    "    load_model, \n",
    "    save_model, \n",
    "    epochs,\n",
    "    checkpoint_path,\n",
    "    loss_data_path,\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test set\n",
    "test = Dataset(torch.from_numpy(data_load['pred_inp_seq']), torch.from_numpy(data_load['pred_tar_seq']).long())\n",
    "test_loader = data.DataLoader(test, batch_size=1, shuffle=False)\n",
    "model = LSTM(src_vocab_size=768, tgt_vocab_size=7).to(device)\n",
    "predict_lstm(model, checkpoint_path, test_loader, rnn_preds, rnn_preds_readable, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O-glyc one hot encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "#epochs = 50\n",
    "load_model = False  # set True to load a checkpoint before resuming training\n",
    "save_model = True  # set True to make a checkpoint of the model after each epoch\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# load data\n",
    "data_load = np.load(datapath / \"one_hot_o_glyc_data.npz\", allow_pickle=True)\n",
    "\n",
    "train = Dataset(torch.from_numpy(data_load['train_inp_seq']), torch.from_numpy(data_load['train_tar_seq']).long())\n",
    "train_loader = data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val = Dataset(torch.from_numpy(data_load['val_inp_seq']), torch.from_numpy(data_load['val_tar_seq']).long())\n",
    "val_loader = data.DataLoader(val, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# set paths for model checkpoints\n",
    "checkpoint_path = datapath / 'model_rnn_o_one_hot.pt'\n",
    "loss_data_path = datapath / 'loss_data_o_one_hot.npz'\n",
    "\n",
    "# path for saving predictions\n",
    "rnn_preds = datapath / \"rnn_pred_o_one_hot.npz\"\n",
    "rnn_preds_readable = datapath / \"rnn_preds_translated_o_one_hot.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(src_vocab_size=30, tgt_vocab_size=7).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0)\n",
    "lr_sched = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_lstm(\n",
    "    model, \n",
    "    optimizer, \n",
    "    lr_sched, \n",
    "    criterion, \n",
    "    load_model, \n",
    "    save_model, \n",
    "    epochs,\n",
    "    checkpoint_path,\n",
    "    loss_data_path,\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test set\n",
    "test = Dataset(torch.from_numpy(data_load['pred_inp_seq']), torch.from_numpy(data_load['pred_tar_seq']).long())\n",
    "test_loader = data.DataLoader(test, batch_size=1, shuffle=False)\n",
    "predict_lstm(model, checkpoint_path, test_loader, rnn_preds, rnn_preds_readable, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-glyc one hot encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "#epochs = 50\n",
    "load_model = False  # set True to load a checkpoint before resuming training\n",
    "save_model = True  # set True to make a checkpoint of the model after each epoch\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "# load data\n",
    "data_load = np.load(datapath / \"one_hot_n_glyc_data.npz\", allow_pickle=True)\n",
    "\n",
    "train = Dataset(torch.from_numpy(data_load['train_inp_seq']), torch.from_numpy(data_load['train_tar_seq']).long())\n",
    "train_loader = data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val = Dataset(torch.from_numpy(data_load['val_inp_seq']), torch.from_numpy(data_load['val_tar_seq']).long())\n",
    "val_loader = data.DataLoader(val, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# set paths for model checkpoints\n",
    "checkpoint_path = datapath / 'model_rnn_n_one_hot.pt'\n",
    "loss_data_path = datapath / 'loss_data_n_one_hot.npz'\n",
    "\n",
    "# path for saving predictions\n",
    "rnn_preds = datapath / \"rnn_pred_n_one_hot.npz\"\n",
    "rnn_preds_readable = datapath / \"rnn_preds_translated_n_one_hot.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(src_vocab_size=30, tgt_vocab_size=7).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0)\n",
    "lr_sched = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_lstm(\n",
    "    model, \n",
    "    optimizer, \n",
    "    lr_sched, \n",
    "    criterion, \n",
    "    load_model, \n",
    "    save_model, \n",
    "    epochs,\n",
    "    checkpoint_path,\n",
    "    loss_data_path,\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test set\n",
    "test = Dataset(torch.from_numpy(data_load['pred_inp_seq']), torch.from_numpy(data_load['pred_tar_seq']).long())\n",
    "test_loader = data.DataLoader(test, batch_size=1, shuffle=False)\n",
    "model = LSTM(src_vocab_size=30, tgt_vocab_size=7).to(device)\n",
    "predict_lstm(model, checkpoint_path, test_loader, rnn_preds, rnn_preds_readable, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N glyc\n",
    "cm_plot_labels = [\"No N glycosyalation\", \"N glcyosylation\"]\n",
    "\n",
    "data_load = np.load(datapath / \"rnn_pred_n_one_hot.npz\", allow_pickle=True)\n",
    "targets = data_load[\"tar\"]\n",
    "preds = data_load[\"pred\"]\n",
    "mask = preds >= 0.5\n",
    "preds_binary = mask.astype(int)\n",
    "\n",
    "cm_analysis(targets, preds_binary, cm_plot_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_load = np.load(datapath / \"rnn_pred_n.npz\", allow_pickle=True)\n",
    "targets = data_load[\"tar\"]\n",
    "preds = data_load[\"pred\"]\n",
    "mask = preds >= 0.5\n",
    "preds_binary = mask.astype(int)\n",
    "\n",
    "\n",
    "cm_analysis(targets, preds_binary, cm_plot_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O glyc\n",
    "cm_plot_labels = [\"No O glycosyalation\", \"O glcyosylation\"]\n",
    "\n",
    "data_load = np.load(datapath / \"rnn_pred_o_one_hot.npz\", allow_pickle=True)\n",
    "targets = data_load[\"tar\"]\n",
    "preds = data_load[\"pred\"]\n",
    "mask = preds >= 0.5\n",
    "preds_binary = mask.astype(int)\n",
    "\n",
    "cm_analysis(targets, preds_binary, cm_plot_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_load = np.load(datapath / \"rnn_pred_o.npz\", allow_pickle=True)\n",
    "targets = data_load[\"tar\"]\n",
    "preds = data_load[\"pred\"]\n",
    "mask = preds >= 0.5\n",
    "preds_binary = mask.astype(int)\n",
    "\n",
    "cm_analysis(targets, preds_binary, cm_plot_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NetOglyc and NetNglyc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These programs were downloaded from https://services.healthtech.dtu.dk/service.php?NetOGlyc-4.0 and https://services.healthtech.dtu.dk/service.php?NetNGlyc-1.0 and run locally on the test set used in the above cells. Their installation requires a bit of tinkering, and so they are not included in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_output_file(path, new_protein_regex, glyc_pred_regex, no_match_string):\n",
    "    \"\"\"Parses the output file of netN or netO glyc tools and transforms them into a usable format.\"\"\"\n",
    "    net_preds = list()\n",
    "    tar_seq = None\n",
    "    parse_flag = False\n",
    "    with open(path, \"r\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            new_protein = re.search(new_protein_regex, line)\n",
    "            if new_protein is not None:\n",
    "                length = int(new_protein.group(1))\n",
    "                tar_seq = np.zeros(length)\n",
    "            glyc_pred = re.search(glyc_pred_regex, line)\n",
    "            if glyc_pred is not None:\n",
    "                position = int(glyc_pred.group(1)) - 1\n",
    "                score = float(glyc_pred.group(2))\n",
    "                tar_seq[position] = score\n",
    "                parse_flag = True\n",
    "            if line[0:4] == \"----\" and parse_flag is True:\n",
    "                net_preds.append(tar_seq)\n",
    "                tar_seq = None\n",
    "                parse_flag = False\n",
    "            elif line == no_match_string:\n",
    "                net_preds.append(tar_seq)\n",
    "                tar_seq = None\n",
    "                parse_flag = False\n",
    "    return np.array(net_preds, dtype=object)\n",
    "\n",
    "\n",
    "ROOT_DIR = Path.cwd()\n",
    "#netNglyc_path = ROOT_DIR / 'data' / 'netNglyc_out.txt'\n",
    "#netOglyc_path = ROOT_DIR / 'data' / 'netOglyc_out.txt'\n",
    "rnn_path_n = ROOT_DIR / 'data' / 'rnn_pred_n.npz'\n",
    "rnn_path_o = ROOT_DIR / 'data' / 'rnn_pred_o.npz'\n",
    "rnn_one_hot_n = datapath / \"rnn_pred_n_one_hot.npz\"\n",
    "rnn_one_hot_o = datapath / \"rnn_pred_o_one_hot.npz\"\n",
    "\n",
    "\n",
    "preds_path_o = ROOT_DIR / \"data\" / \"FFN_O_glyc_predictions.npz\"\n",
    "preds_path_n = ROOT_DIR / \"data\" / \"FFN_N_glyc_predictions.npz\"\n",
    "trans_pred_path_o = ROOT_DIR / \"data\" / \"Transformer_o_glyc_predictions.npz\"\n",
    "trans_pred_path_n = ROOT_DIR / \"data\" / \"Transformer_n_glyc_predictions.npz\"\n",
    "\n",
    "# grab prediction scores from netNglyc output\n",
    "#no_match_string = \"No sites predicted in this sequence.\"\n",
    "#new_protein_regex = r\"Name:\\s+test_seq_\\d+\\s+Length:\\s+(\\d+)\"\n",
    "#glyc_pred_regex = r\"test_seq_\\d+\\s+(\\d+)\\s\\w+\\s+(\\d{1}.\\d+)\"\n",
    "#netN_preds = parse_output_file(netNglyc_path, new_protein_regex, glyc_pred_regex, no_match_string)\n",
    "\n",
    "# grab prediction scores from netOglyc output\n",
    "#no_match_string = \"No sites predicted in this sequence.\"  # TODO figure out the no hit message and put it here\n",
    "#new_protein_regex = r\"Name:\\s+ts_\\d+\\s+Length:\\s+(\\d+)\"\n",
    "#glyc_pred_regex = r\"ts_\\d+\\s+\\w+\\s+(\\d+)\\s+(\\d\\.\\d+)\"\n",
    "#netO_preds = parse_output_file(netOglyc_path, new_protein_regex, glyc_pred_regex, no_match_string)\n",
    "\n",
    "# grab predictions scores from LSTM\n",
    "rnn_preds_n = np.load(rnn_path_n, allow_pickle=True)[\"pred\"]\n",
    "rnn_tars_n = np.load(rnn_path_n, allow_pickle=True)[\"tar\"]\n",
    "rnn_preds_o = np.load(rnn_path_o, allow_pickle=True)[\"pred\"]\n",
    "rnn_tars_o = np.load(rnn_path_o, allow_pickle=True)[\"tar\"]\n",
    "\n",
    "rnn_preds_one_hot_n = np.load(rnn_one_hot_n, allow_pickle=True)[\"pred\"]\n",
    "rnn_tars_one_hot_n = np.load(rnn_one_hot_n, allow_pickle=True)[\"tar\"]\n",
    "rnn_preds_one_hot_o = np.load(rnn_one_hot_o, allow_pickle=True)[\"pred\"]\n",
    "rnn_tars_one_hot_o = np.load(rnn_one_hot_o, allow_pickle=True)[\"tar\"]\n",
    "\n",
    "# grab FNN scores\n",
    "data_load_n = np.load(preds_path_n, allow_pickle=True)\n",
    "ffn_preds_n = data_load_n['glyc_prob']\n",
    "target_values_n = data_load_n['tar_seq'].astype(int)\n",
    "\n",
    "data_load_o = np.load(preds_path_o, allow_pickle=True)\n",
    "ffn_preds_o = data_load_o['glyc_prob']\n",
    "target_values_o = data_load_o['tar_seq'].astype(int)\n",
    "\n",
    "# grab trans tars\n",
    "trans_pred_o = np.load(trans_pred_path_o, allow_pickle=True)[\"pred\"]\n",
    "trans_pred_n = np.load(trans_pred_path_n, allow_pickle=True)[\"pred\"]\n",
    "trans_tar_o = np.load(trans_pred_path_o, allow_pickle=True)[\"tar_seq\"]\n",
    "trans_tar_n = np.load(trans_pred_path_n, allow_pickle=True)[\"tar_seq\"]\n",
    "\n",
    "\n",
    "# flatten arrays to allow for ROC AUC calc\n",
    "#targets_n = np.hstack(targets_n)\n",
    "#targets_o = np.hstack(targets_o)\n",
    "#netN_preds = np.hstack(netN_preds)\n",
    "#netO_preds = np.hstack(netO_preds)\n",
    "#trans_pred_o = np.hstack([x[:-1] for x in trans_pred_o])\n",
    "#trans_pred_n = np.hstack([x[:-1] for x in trans_pred_n])\n",
    "\n",
    "# netN and netO auc\n",
    "#fpr_net_N, tpr_net_N, thresh_net_N = metrics.roc_curve(targets_n, netN_preds, pos_label=1)\n",
    "#auc_net_N = metrics.auc(fpr_net_N, tpr_net_N)\n",
    "\n",
    "#fpr_net_O, tpr_net_O, thresh_net_O = metrics.roc_curve(targets_o, netO_preds, pos_label=1)\n",
    "#auc_net_O = metrics.auc(fpr_net_O, tpr_net_O)\n",
    "\n",
    "# LSTM aucs\n",
    "fpr_rnn_N, tpr_rnn_N, thresh_rnn_N = metrics.roc_curve(rnn_tars_n, rnn_preds_n, pos_label=1)\n",
    "auc_rnn_N = metrics.auc(fpr_rnn_N, tpr_rnn_N)\n",
    "\n",
    "fpr_rnn_O, tpr_rnn_O, thresh_rnn_O = metrics.roc_curve(rnn_tars_o, rnn_preds_o, pos_label=1)\n",
    "auc_rnn_O = metrics.auc(fpr_rnn_O, tpr_rnn_O)\n",
    "\n",
    "fpr_rnn_one_hot_N, tpr_rnn_one_hot_N, thresh_rnn_N = metrics.roc_curve(rnn_tars_one_hot_n, rnn_preds_one_hot_n, pos_label=1)\n",
    "auc_rnn_one_hot_N = metrics.auc(fpr_rnn_one_hot_N, tpr_rnn_one_hot_N)\n",
    "\n",
    "fpr_rnn_one_hot_O, tpr_rnn_one_hot_O, thresh_rnn_O = metrics.roc_curve(rnn_tars_one_hot_o, rnn_preds_one_hot_o, pos_label=1)\n",
    "auc_rnn_one_hot_O = metrics.auc(fpr_rnn_one_hot_O, tpr_rnn_one_hot_O)\n",
    "\n",
    "# FNN aucs\n",
    "fpr_ffn_N, tpr_ffn_N, thresh_ffn_N = metrics.roc_curve(target_values_n, ffn_preds_n, pos_label=1)\n",
    "auc_ffn_N = metrics.auc(fpr_ffn_N, tpr_ffn_N)\n",
    "\n",
    "fpr_ffn_O, tpr_ffn_O, thresh_ffn_O = metrics.roc_curve(target_values_o, ffn_preds_o, pos_label=1)\n",
    "auc_ffn_O = metrics.auc(fpr_ffn_O, tpr_ffn_O)\n",
    "\n",
    "# trans aucs\n",
    "fpr_trans_O, tpr_trans_O, thresh_trans_O = metrics.roc_curve(trans_tar_o, trans_pred_o, pos_label=1)\n",
    "auc_trans_O = metrics.auc(fpr_trans_O, tpr_trans_O)\n",
    "\n",
    "fpr_trans_N, tpr_trans_N, thresh_trans_N = metrics.roc_curve(trans_tar_n, trans_pred_n, pos_label=1)\n",
    "auc_trans_N = metrics.auc(fpr_trans_N, tpr_trans_N)\n",
    "\n",
    "xmin, xmax, ymin, ymax = 0.0, 0.7, 0.3, 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-plot\n",
    "ax = plt.subplot()\n",
    "#ax.plot(fpr_net_N, tpr_net_N)\n",
    "ax.plot(fpr_rnn_N, tpr_rnn_N)\n",
    "ax.plot(fpr_ffn_N, tpr_ffn_N)\n",
    "ax.plot(fpr_trans_N, tpr_trans_N)\n",
    "ax.plot(fpr_rnn_one_hot_N, tpr_rnn_one_hot_N)\n",
    "ax.plot(np.linspace(0, 1), np.linspace(0, 1), \"--\")\n",
    "\n",
    "#ax.legend([\"netNglyc-1.0, AUC={}\".format(round(auc_net_N, 4)),\n",
    "#            \"BERT + LSTM, AUC={}\".format(round(auc_rnn_N, 4)),\n",
    "#            \"BERT + FFN, AUC={}\".format(round(auc_ffn_N, 4)),\n",
    "#            \"BERT + Transf. decoder, AUC={}\".format(round(auc_trans_N, 4)),\n",
    "#            \"LSTM, AUC={}\".format(round(auc_rnn_one_hot_N, 4))],\n",
    "#          prop={\"size\": 12})\n",
    "ax.legend([\"BERT + LSTM, AUC={}\".format(round(auc_rnn_N, 4)),\n",
    "            \"BERT + FFN, AUC={}\".format(round(auc_ffn_N, 4)),\n",
    "            \"BERT + Transf. decoder, AUC={}\".format(round(auc_trans_N, 4)),\n",
    "            \"LSTM, AUC={}\".format(round(auc_rnn_one_hot_N, 4))],\n",
    "          prop={\"size\": 12})\n",
    "\n",
    "ax.axis([xmin,xmax,ymin,ymax])\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.title(\"ROC curves for N-linked glycosylation\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.show()\n",
    "plt.savefig('N_glyc.png', dpi=800)\n",
    "#plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O-plot\n",
    "ax = plt.subplot()\n",
    "#ax.plot(fpr_net_O, tpr_net_O)\n",
    "ax.plot(fpr_rnn_O, tpr_rnn_O)\n",
    "ax.plot(fpr_ffn_O, tpr_ffn_O)\n",
    "ax.plot(fpr_trans_O, tpr_trans_O)\n",
    "ax.plot(fpr_rnn_one_hot_O, tpr_rnn_one_hot_O)\n",
    "ax.plot(np.linspace(0, 1), np.linspace(0, 1), \"--\")\n",
    "\n",
    "#ax.legend([\"netOglyc-3.1, AUC={}\".format(round(auc_net_O, 4)),\n",
    "#            \"BERT + LSTM, AUC={}\".format(round(auc_rnn_O, 4)),\n",
    "#            \"BERT + FFN, AUC={}\".format(round(auc_ffn_O, 4)),\n",
    "#            \"BERT + Transf. decoder, AUC={}\".format(round(auc_trans_O, 4)),\n",
    "#            \"LSTM, AUC={}\".format(round(auc_rnn_one_hot_O, 4))],\n",
    "#          prop={\"size\": 12})\n",
    "\n",
    "\n",
    "ax.legend([\"BERT + LSTM, AUC={}\".format(round(auc_rnn_O, 4)),\n",
    "            \"BERT + FFN, AUC={}\".format(round(auc_ffn_O, 4)),\n",
    "            \"BERT + Transf. decoder, AUC={}\".format(round(auc_trans_O, 4)),\n",
    "            \"LSTM, AUC={}\".format(round(auc_rnn_one_hot_O, 4))],\n",
    "          prop={\"size\": 12})\n",
    "\n",
    "ax.axis([xmin,xmax,ymin,ymax])\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.title(\"ROC curves for O-linked glycosylation\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.show()\n",
    "plt.savefig('O_glyc.png', dpi=800)\n",
    "#plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
